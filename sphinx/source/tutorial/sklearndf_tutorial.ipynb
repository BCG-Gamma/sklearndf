{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "def _set_paths() -> None:\n",
    "    # set the correct path when launched from within PyCharm\n",
    "\n",
    "    module_paths = [\"pytools\", \"sklearndf\"]\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    if 'cwd' not in globals():\n",
    "        # noinspection PyGlobalUndefined\n",
    "        global cwd\n",
    "        cwd = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "        os.chdir(cwd)   \n",
    "    print(f\"working dir is '{os.getcwd()}'\")\n",
    "    for module_path in module_paths:\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, os.path.abspath(f\"{cwd}/{os.pardir}/{module_path}/src\"))\n",
    "        print(f\"added `{sys.path[0]}` to python paths\")\n",
    "        \n",
    "def _ignore_warnings():\n",
    "    # ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "    \n",
    "    # ignore a useless LGBM warning\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "\n",
    "_set_paths()\n",
    "_ignore_warnings()\n",
    "\n",
    "del _set_paths, _ignore_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T08:31:08.171058Z",
     "start_time": "2019-07-26T08:31:08.167059Z"
    }
   },
   "source": [
    "# Scikit-learn and data frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The `sklearndf` package enhances scikit-learn for advanced support of data frames.\n",
    "\n",
    "It addresses a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names.\n",
    "\n",
    "`sklearndf` enhances scikit-learn's estimators to:\n",
    "- return data frames as results of transformations, preserving feature names as the column index\n",
    "- add additional estimatgor properties to enable tracing a feature name back to its original input feature; this is especially useful for transformers that create new features (e.g., one-hot encode), and for pipelines that include such transformers \n",
    "\n",
    "Using `sklearndf` is very simple: Append `DF` at the end of scikit-learn class names and you will get enhanced data frame support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:38.950751Z",
     "start_time": "2019-07-30T18:53:36.816398Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearndf.classification import RandomForestClassifierDF\n",
    "from sklearndf.pipeline import PipelineDF, RegressorPipelineDF\n",
    "from sklearndf.regression import RandomForestRegressorDF\n",
    "from sklearndf.regression.extra import LGBMRegressorDF\n",
    "from sklearndf.transformation import ColumnTransformerDF, OneHotEncoderDF, SimpleImputerDF\n",
    "from sklearndf.transformation.extra import BorutaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:38.991698Z",
     "start_time": "2019-07-30T18:53:38.953342Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "housing_features_df, housing_target_sr = fetch_openml(data_id=42165, return_X_y=True, as_frame=True)\n",
    "housing_features_df = housing_features_df.drop([\"Id\", \"YrSold\", \"MoSold\", \"MSSubClass\", \"MiscVal\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set includes categorical features, e.g., garage types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.002353Z",
     "start_time": "2019-07-30T18:53:38.993328Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "housing_df[\"GarageType\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T13:42:19.065036Z",
     "start_time": "2019-07-26T13:42:19.058033Z"
    }
   },
   "source": [
    "Let us build a preprocessing pipeline which:\n",
    "\n",
    "- for categorical variables fills missing values with the string 'nan' and then one-hot encodes\n",
    "- for numerical values fills missing values using median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.044336Z",
     "start_time": "2019-07-30T18:53:39.019314Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = housing_features_df.select_dtypes(object).columns\n",
    "numerical_features = housing_features_df.select_dtypes(pd.np.number).columns\n",
    "\n",
    "categorical_features, numerical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrasting a scikit-learn and sklearndf pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build the preprocessing pipeline with native scikit-learn transformers.\n",
    "This is achievable with a few lines of code; however does not allow us to keep track of feature names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.055313Z",
     "start_time": "2019-07-30T18:53:39.047314Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_numeric = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "\n",
    "preprocessing_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(missing_values=None, strategy='constant', fill_value='<unknown>')),\n",
    "        ('one-hot', OneHotEncoder(sparse=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', preprocessing_numeric, numerical_features),\n",
    "        ('categorical', preprocessing_categorical, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.134309Z",
     "start_time": "2019-07-30T18:53:39.082355Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessing.fit_transform(X=housing_features_df, y=housing_target_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The strength of `sklearndf` is to maintain the scikit-learn conventions and expressivity, and to also preserve dataframes, hence keeping track of the feature names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An sklearndf pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The convention in `sklearndf` is to append `DF` at the end of each corresponding scikit-learn class. \n",
    "For instance, to reproduce the above example, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.153308Z",
     "start_time": "2019-07-30T18:53:39.145308Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_numeric_df = SimpleImputerDF(strategy=\"median\", add_indicator=True)\n",
    "\n",
    "preprocessing_categorical_df = PipelineDF(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputerDF(missing_values=None, strategy='constant', fill_value='<unknown>')),\n",
    "        ('one-hot', OneHotEncoderDF(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_df = ColumnTransformerDF(\n",
    "    transformers=[\n",
    "        ('categorical', preprocessing_categorical_df, categorical_features),\n",
    "        ('numeric', preprocessing_numeric_df, numerical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:53:39.249358Z",
     "start_time": "2019-07-30T18:53:39.180306Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = preprocessing_df.fit_transform(X=housing_features_df, y=housing_target_sr)\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The `~sklearndf.transformation.ColumnTransformerDF.features_original_` attribute returns a series mapping the output columns (the series' index) to the input columns (the series' values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:35.436833Z",
     "start_time": "2019-07-30T18:54:35.431830Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_df.features_original_.to_frame().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can therefore easily select all output features generated from a given input feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:47.270660Z",
     "start_time": "2019-07-30T18:54:47.254638Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "garage_type_derivatives = preprocessing_df.features_original_ == \"GarageType\"\n",
    "\n",
    "transformed_df.loc[:, garage_type_derivatives].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T08:36:43.003042Z",
     "start_time": "2019-07-26T08:36:42.733047Z"
    }
   },
   "source": [
    "## Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for transformers, scikit-learn regressors and classifiers have a `sklearndf` sibling obtained by appending `DF` to the class name, and the API remains the same. The result of any predict and decision function will be returned as a pandas series (single output) or data frame (class probabilities or multi-output).\n",
    "\n",
    "For a random forest regressor we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:49.180964Z",
     "start_time": "2019-07-30T18:54:48.757981Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# a simplified features vector (we will use a pipeline for more sophisticated pre-processing further down)\n",
    "numerical_features_df = housing_features_df.loc[:, numerical_features].fillna(0)\n",
    "\n",
    "df_numerical_train, df_numerical_test, y_train, y_test = train_test_split(\n",
    "    numerical_features_df,\n",
    "    housing_target_sr,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_forest_regressor_df = RandomForestRegressorDF(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-3\n",
    ")\n",
    "\n",
    "random_forest_regressor_df.fit(X=df_numerical_train, y=y_train)\n",
    "random_forest_regressor_df.score(X=df_numerical_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_df.predict(df_numerical_test.iloc[:10]).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:49.188965Z",
     "start_time": "2019-07-30T18:54:49.182965Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor_df.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:49.443565Z",
     "start_time": "2019-07-30T18:54:49.431562Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor_df.set_params(max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The underlying scikit-learn regressor is stored in the `native_estimator` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:49.895830Z",
     "start_time": "2019-07-30T18:54:49.889831Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor_df.native_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Property `is_fitted` tells if the regressor is fitted, and -- for fitted estimators -- property `features_in_` returns the names of the ingoing features as a pandas index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:50.523346Z",
     "start_time": "2019-07-30T18:54:50.518398Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor_df.is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:50.349489Z",
     "start_time": "2019-07-30T18:54:50.345466Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor_df.features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "Classifiers follow the same logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:50.833368Z",
     "start_time": "2019-07-30T18:54:50.826362Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# we create for house prices house below 100k, below 200k, and above 200k for multi-label classification\n",
    "y_classes = housing_target_sr.apply(lambda x: '>=200k' if x >= 200000 else '>=100k' if x >= 100000 else '<100k')\n",
    "\n",
    "df_numerical_train, df_numerical_test, y_classification_train, y_classification_test = train_test_split(\n",
    "    numerical_features_df,\n",
    "    y_classes,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:51.432224Z",
     "start_time": "2019-07-30T18:54:51.341154Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_classifier_df = RandomForestClassifierDF(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-3\n",
    ")\n",
    "random_forest_classifier_df.fit(df_numerical_train, y_classification_train)\n",
    "random_forest_classifier_df.score(df_numerical_test, y_classification_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_df.predict(df_numerical_test.iloc[:10]).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_df.predict_proba(df_numerical_test.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_df.predict_log_proba(df_numerical_test.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T08:37:37.633663Z",
     "start_time": "2019-07-26T08:37:37.618669Z"
    },
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can combine the above steps to build a full predictive pipeline. `sklearndf` provides two useful, specialised pipeline objects for this, `~sklearndf.pipeline.RegressorPipelineDF` and `~sklearndf.pipeline.ClassifierPipelineDF`. Both implement a special two-step pipeline with one pre-processing step and one prediction step, while staying compatible with the general sklearn pipeline idiom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:51.520150Z",
     "start_time": "2019-07-30T18:54:51.513148Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline_df = RegressorPipelineDF(\n",
    "    preprocessing=preprocessing_df,\n",
    "    regressor=RandomForestRegressorDF(\n",
    "        n_estimators=1000,\n",
    "        max_features=2/3,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:51.874135Z",
     "start_time": "2019-07-30T18:54:51.522150Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test, y_train, y_test = train_test_split(housing_features_df, housing_target_sr, random_state=42)\n",
    "pipeline_df.fit(df_train, y_train)\n",
    "pipeline_df.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "`sklearndf` also provides some additional estimators developed by Gamma or third parties, which are useful additions to the scikit-learn repertoire, and which follow the scikit-learn idiom. These are provided in `.extra` modules, such as\n",
    "\n",
    "- `sklearndf.regression.extra.LGBMRegressorDF`\n",
    "- `sklearndf.transformation.extra.BorutaDF`\n",
    "- `sklearndf.transformation.extra.OutlierRemoverDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:54:52.046129Z",
     "start_time": "2019-07-30T18:54:51.876135Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm_df = LGBMRegressorDF(n_estimators=100, max_depth=8)\n",
    "lgbm_df.fit(df_numerical_train, y_train)\n",
    "lgbm_df.predict(df_numerical_test.iloc[:10]).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "`Boruta <https://www.jstatsoft.org/article/view/v036i11>`_ is a smart feature selection method to eliminate all features whose predictive power is not better than random noise.\n",
    "\n",
    "The `sklearndf.transformation.extra.BorutaDF` transformer provides easy access to this powerful method. The basis of this is a tree-based learner, usually a random forest.\n",
    "\n",
    "For the random forest, we rely on default parameters but set the maximum tree depth to 5 (for Boruta, setting a depth between 3 and 7 is highly recommended and depends on the number of features and expected complexity of the feature/target interactions). The number of trees is automatically managed by the Boruta feature selector (argument ``n_estimators=\"auto\"``).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_pipeline = PipelineDF(\n",
    "    steps=[\n",
    "        ('preprocess', preprocessing_df),\n",
    "        ('boruta', BorutaDF(\n",
    "            estimator=RandomForestRegressorDF(max_depth=5, n_jobs=-3), \n",
    "            n_estimators=\"auto\", \n",
    "            random_state=42,\n",
    "            verbose=2\n",
    "        )),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_pipeline.fit(X=housing_features_df, y=housing_target_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta is implemented as an sklearn transformer; its output features are all features that passed the Boruta test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_pipeline.features_out_.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "`sklearndf` allows us to trace outgoing features back to the original features from which they were derived, using the `~sklearndf.TransformerDF.features_original_` property. This is useful here as we want to know which features to eliminate before putting them into the pipeline.\n",
    "\n",
    "In our example, feature `BsmtQual_Ex` is a derivative of feature `BsmtQual`, obtained through one-hot encoding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_pipeline.features_original_.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to obtain all features we want to select from the original data set, we can select the unique ingoing features from the original feature mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_pipeline.features_original_.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
